{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8297ac6eb86b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-8297ac6eb86b>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mBuild\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcomputation\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         '''\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-8297ac6eb86b>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_test_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             iterator = tf.data.Iterator.from_structure(train_data.output_types, \n\u001b[0;32m    112\u001b[0m                                                    train_data.output_shapes)\n",
      "\u001b[1;32m<ipython-input-28-8297ac6eb86b>\u001b[0m in \u001b[0;36mread_test_images\u001b[1;34m(dataset_path, mode, batch_size)\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.bmp'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.BMP'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                     \u001b[0mimagepaths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                     \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "#taken from stanford tensorflow tutorial for cnn\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time \n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#train data class normal 38 class bad 280 \n",
    "#test data class normal 10 class bad 20\n",
    "import utils\n",
    "MODE = 'folder' # or 'file', if you choose a plain text file (see above).\n",
    "DATASET_PATH = 'E:\\HolisticClassification\\Dataset' # the dataset file or root folder path.\n",
    "TEST_DATASET_PATH = 'E:\\HolisticClassification\\Dataset\\Test'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Image Parameters\n",
    "N_CLASSES = 2 # CHANGE HERE, total number of classes\n",
    "IMG_HEIGHT = 32 # CHANGE HERE, the image height to be resized to\n",
    "IMG_WIDTH = 32 # CHANGE HERE, the image width to be resized to\n",
    "CHANNELS = 3 # The 3 color channels, change to 1 if grayscale\n",
    "def read_images(dataset_path = DATASET_PATH, mode = MODE, batch_size = BATCH_SIZE):\n",
    "    imagepaths, labels = list(), list()\n",
    "    if mode == 'folder':\n",
    "        label = 0\n",
    "        classes = [f.path for f in os.scandir(DATASET_PATH) if f.is_dir() ] \n",
    "        for c in classes:\n",
    "            c_dir = os.path.join(dataset_path, c)\n",
    "            walk = os.walk(c_dir).__next__()\n",
    "            for sample in walk[2]:\n",
    "                if sample.endswith('.bmp') or sample.endswith('.BMP'):\n",
    "                    imagepaths.append(os.path.join(c_dir, sample))\n",
    "                    labels.append(label)\n",
    "            label += 1\n",
    "    else:\n",
    "        raise Exception(\"Unknown mode.\")\n",
    "    resized_image_array=[]\n",
    "\n",
    "    for image_loc in imagepaths:\n",
    "        image_decoded = tf.image.decode_bmp(tf.gfile.FastGFile(image_loc, 'rb').read(),channels=CHANNELS)\n",
    "        resized_image = tf.reshape(tf.image.resize_images(image_decoded, [IMG_HEIGHT,IMG_HEIGHT]),[1,IMG_HEIGHT*IMG_HEIGHT*CHANNELS])\n",
    "        resized_image_array.append(resized_image)\n",
    "    resized_image_array = tf.reshape(tf.stack(resized_image_array),[len(imagepaths),IMG_HEIGHT*IMG_HEIGHT*CHANNELS])\n",
    "    # Normalize\n",
    "    #image = image * 1.0/127.5 - 1.0\n",
    "    image = tf.convert_to_tensor(resized_image_array,dtype=tf.float32)\n",
    "    labels = tf.convert_to_tensor(labels,dtype=tf.float32);\n",
    "    ilabels = tf.reshape(labels,[len(imagepaths),1])\n",
    "    train_data = tf.concat([image, ilabels], 1)\n",
    "    train_data = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "    train_data = train_data.shuffle(128)\n",
    "    train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "    # Create batches\n",
    "   # X, Y = tf.train.batch([image, label], batch_size=batch_size, capacity=batch_size * 8, num_threads=4)\n",
    "    return train_data\n",
    "\n",
    "def read_test_images(dataset_path = TEST_DATASET_PATH, mode = MODE, batch_size = BATCH_SIZE):\n",
    "    imagepaths =  list()\n",
    "    if mode == 'folder':\n",
    "        classes = [f.path for f in os.scandir(DATASET_PATH) if f.is_dir() ] \n",
    "        for c in classes:\n",
    "            c_dir = os.path.join(dataset_path, c)\n",
    "            walk = os.walk(c_dir).__next__()\n",
    "            for sample in walk[2]:\n",
    "                if sample.endswith('.bmp') or sample.endswith('.BMP'):\n",
    "                    imagepaths.append(os.path.join(c_dir, sample))\n",
    "    else:\n",
    "        raise Exception(\"Unknown mode.\")\n",
    "\n",
    "    imagepaths = tf.convert_to_tensor(imagepaths, dtype=tf.string)\n",
    "    # Build a TF Queue, shuffle data\n",
    "    image, label = tf.train.slice_input_producer(imagepaths,\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    # Read images from disk\n",
    "    \n",
    "    image = tf.image.decode_bmp(image, channels=CHANNELS)\n",
    "\n",
    "    # Resize images to a common size\n",
    "    image = tf.image.resize_images(image, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "    # Normalize\n",
    "    #image = image * 1.0/127.5 - 1.0\n",
    "    test = image\n",
    "    test_data = tf.data.Dataset.from_tensor_slices(test)\n",
    "    test_data = test_data.batch(BATCH_SIZE)\n",
    "    # Create batches\n",
    "    # Create batches\n",
    "   # X, Y = tf.trai.batch([image, label], batch_size=batch_size, capacity=batch_size * 8, num_threads=4)\n",
    "    return test_data\n",
    "\n",
    "\n",
    "class ConvNet(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.keep_prob = tf.constant(0.75)\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, \n",
    "                                trainable=False, name='global_step')\n",
    "        self.n_classes = 2\n",
    "        self.skip_step = 20\n",
    "        self.n_test = 30\n",
    "        self.training=False\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope('data'):\n",
    "            train_data = read_images()\n",
    "            test_data = read_test_images()\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, IMG_WIDTH, IMG_HEIGHT, CHANNELS])\n",
    "            # reshape the image to make it work with tf.nn.conv2d\n",
    "\n",
    "            self.train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "            self.test_init = iterator.make_initializer(test_data)    # initializer for train_data\n",
    "\n",
    "    def inference(self):\n",
    "        conv1 = tf.layers.conv2d(inputs=self.img,\n",
    "                                  filters=32,\n",
    "                                  kernel_size=[5, 5],\n",
    "                                  padding='SAME',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name='conv1')\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2,\n",
    "                                        name='pool1')\n",
    "\n",
    "        conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                                  filters=64,\n",
    "                                  kernel_size=[5, 5],\n",
    "                                  padding='SAME',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  name='conv2')\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2,\n",
    "                                        name='pool2')\n",
    "\n",
    "        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n",
    "        pool2 = tf.reshape(pool2, [-1, feature_dim])\n",
    "        fc = tf.layers.dense(pool2, 128, activation=tf.nn.relu, name='fc')\n",
    "        dropout = tf.layers.dropout(fc, \n",
    "                                    self.keep_prob, \n",
    "                                    training=self.training, \n",
    "                                    name='dropout')\n",
    "        self.logits = tf.layers.dense(dropout, self.n_classes, name='logits')\n",
    "\n",
    "    def loss(self):\n",
    "        '''\n",
    "        define loss function\n",
    "        use softmax cross entropy with logits as the loss function\n",
    "        compute mean cross entropy, softmax is applied internally\n",
    "        '''\n",
    "        # \n",
    "        with tf.name_scope('loss'):\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
    "    \n",
    "    def optimize(self):\n",
    "        '''\n",
    "        Define training op\n",
    "        using Adam Gradient Descent to minimize cost\n",
    "        '''\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, \n",
    "                                                global_step=self.gstep)\n",
    "\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Create summaries to write on TensorBoard\n",
    "        '''\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def eval(self):\n",
    "        '''\n",
    "        Count the number of right predictions in a batch\n",
    "        '''\n",
    "        with tf.name_scope('predict'):\n",
    "            preds = tf.nn.softmax(self.logits)\n",
    "            correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
    "            self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Build the computation graph\n",
    "        '''\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.eval()\n",
    "        self.summary()\n",
    "\n",
    "    def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init) \n",
    "        self.training = True\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                if (step + 1) % self.skip_step == 0:\n",
    "                    print('Loss at step {0}: {1}'.format(step, l))\n",
    "                step += 1\n",
    "                total_loss += l\n",
    "                n_batches += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        saver.save(sess, 'checkpoints/convnet_layers/mnist-convnet', step)\n",
    "        print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "        return step\n",
    "\n",
    "    def eval_once(self, sess, init, writer, epoch, step):\n",
    "        start_time = time.time()\n",
    "        sess.run(init)\n",
    "        self.training = False\n",
    "        total_correct_preds = 0\n",
    "        try:\n",
    "            while True:\n",
    "                accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
    "                writer.add_summary(summaries, global_step=step)\n",
    "                total_correct_preds += accuracy_batch\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
    "        print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    def train(self, n_epochs):\n",
    "        '''\n",
    "        The train function alternates between training one epoch and evaluating\n",
    "        '''\n",
    "        utils.safe_mkdir('checkpoints')\n",
    "        utils.safe_mkdir('checkpoints/convnet_layers')\n",
    "        writer = tf.summary.FileWriter('./graphs/convnet_layers', tf.get_default_graph())\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_layers/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            step = self.gstep.eval()\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
    "                self.eval_once(sess, self.test_init, writer, epoch, step)\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = ConvNet()\n",
    "    model.build()\n",
    "    model.train(n_epochs=15)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
